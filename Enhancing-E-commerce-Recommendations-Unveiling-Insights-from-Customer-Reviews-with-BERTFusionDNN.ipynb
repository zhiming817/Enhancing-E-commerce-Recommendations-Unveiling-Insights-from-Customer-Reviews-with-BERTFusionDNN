{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":16290,"sourceType":"datasetVersion","datasetId":11827}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Loading Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport re\nimport plotly.express as px\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import LSTM\nfrom keras.layers import GRU\nfrom keras.layers import SimpleRNN\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional\nfrom keras.layers import Embedding\nfrom keras.layers import GlobalAvgPool1D\nfrom keras.layers import GlobalMaxPooling1D\nfrom tensorflow.keras.models import Model\n\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom transformers import BertTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:41:36.844034Z","iopub.execute_input":"2024-02-18T05:41:36.844314Z","iopub.status.idle":"2024-02-18T05:41:55.115679Z","shell.execute_reply.started":"2024-02-18T05:41:36.844288Z","shell.execute_reply":"2024-02-18T05:41:55.114940Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Data Processing","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv')\ncleaned_data = dataset.dropna()\nprint(cleaned_data.isnull().sum())\nprint(cleaned_data.shape)\n\ncolumns_of_interest = ['Division Name','Department Name','Class Name']\ncleaned_data[columns_of_interest].nunique()\n\n# Applying One-Hot Encoding to 'Division Name', 'Department Name', and 'Class Name'\nencoded_data = pd.get_dummies(cleaned_data, columns=['Division Name', 'Department Name']).copy()\n\n# Frequency encoding for 'Class Name'\nfrequency_encoding1 = encoded_data['Class Name'].value_counts(normalize=True)\nfrequency_encoding2 = encoded_data['Clothing ID'].value_counts(normalize=True)\n\n# Mapping the frequencies to the original data\nencoded_data['Class Name_Frequency'] = encoded_data['Class Name'].map(frequency_encoding1)\nencoded_data['Clothing ID_Frequency'] = encoded_data['Clothing ID'].map(frequency_encoding2)\nencoded_data.drop(columns=['Clothing ID','Class Name'],inplace = True)\nencoded_data.head()\n\nml_data = encoded_data.copy()\nml_data = ml_data.drop(columns=['Title', 'Review Text','Unnamed: 0'])\n\nml_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:42:08.882823Z","iopub.execute_input":"2024-02-18T05:42:08.883528Z","iopub.status.idle":"2024-02-18T05:42:09.220305Z","shell.execute_reply.started":"2024-02-18T05:42:08.883488Z","shell.execute_reply":"2024-02-18T05:42:09.219260Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Unnamed: 0                 0\nClothing ID                0\nAge                        0\nTitle                      0\nReview Text                0\nRating                     0\nRecommended IND            0\nPositive Feedback Count    0\nDivision Name              0\nDepartment Name            0\nClass Name                 0\ndtype: int64\n(19662, 11)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   Age  Rating  Recommended IND  Positive Feedback Count  \\\n2   60       3                0                        0   \n3   50       5                1                        0   \n4   47       5                1                        6   \n5   49       2                0                        4   \n6   39       5                1                        1   \n\n   Division Name_General  Division Name_General Petite  \\\n2                   True                         False   \n3                  False                          True   \n4                   True                         False   \n5                   True                         False   \n6                  False                          True   \n\n   Division Name_Initmates  Department Name_Bottoms  Department Name_Dresses  \\\n2                    False                    False                     True   \n3                    False                     True                    False   \n4                    False                    False                    False   \n5                    False                    False                     True   \n6                    False                    False                    False   \n\n   Department Name_Intimate  Department Name_Jackets  Department Name_Tops  \\\n2                     False                    False                 False   \n3                     False                    False                 False   \n4                     False                    False                  True   \n5                     False                    False                 False   \n6                     False                    False                  True   \n\n   Department Name_Trend  Class Name_Frequency  Clothing ID_Frequency  \n2                  False              0.273167               0.012766  \n3                  False              0.058844               0.001271  \n4                  False              0.131574               0.000203  \n5                  False              0.273167               0.012257  \n6                  False              0.202472               0.000915  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Rating</th>\n      <th>Recommended IND</th>\n      <th>Positive Feedback Count</th>\n      <th>Division Name_General</th>\n      <th>Division Name_General Petite</th>\n      <th>Division Name_Initmates</th>\n      <th>Department Name_Bottoms</th>\n      <th>Department Name_Dresses</th>\n      <th>Department Name_Intimate</th>\n      <th>Department Name_Jackets</th>\n      <th>Department Name_Tops</th>\n      <th>Department Name_Trend</th>\n      <th>Class Name_Frequency</th>\n      <th>Clothing ID_Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>60</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.273167</td>\n      <td>0.012766</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>50</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.058844</td>\n      <td>0.001271</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>47</td>\n      <td>5</td>\n      <td>1</td>\n      <td>6</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>0.131574</td>\n      <td>0.000203</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>49</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.273167</td>\n      <td>0.012257</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>39</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>0.202472</td>\n      <td>0.000915</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 3. NLP for text data","metadata":{}},{"cell_type":"code","source":"nlp_data = encoded_data[['Title','Review Text','Recommended IND']].copy()\nnlp_data['Text'] = nlp_data['Title'] + ' ' + nlp_data['Review Text']\nnlp_data = nlp_data[['Text', 'Recommended IND']]\n\n# 1. 加载预训练的BERT模型和分词器\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert = TFBertModel.from_pretrained('bert-base-uncased')\n\n# 2. 去除停用词\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n# 获取标点符号\npunctuations = string.punctuation\n\ndef clean_text(text):\n    # 分词\n    tokens = tokenizer.tokenize(text)\n    # 去除停用词和标点符号\n    tokens = [token for token in tokens if token not in stop_words and token not in punctuations]\n    # 重建文本\n    return ' '.join(tokens)\n\n# 应用清洁函数\nnlp_data['Text'] = nlp_data['Text'].apply(clean_text)\n\n# 3. 分词并统计句子长度\ntoken_lengths = []\nfor text in nlp_data['Text']:\n    tokens = tokenizer.encode(text, add_special_tokens=True)\n    token_lengths.append(len(tokens))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:42:13.643101Z","iopub.execute_input":"2024-02-18T05:42:13.643948Z","iopub.status.idle":"2024-02-18T05:43:31.504416Z","shell.execute_reply.started":"2024-02-18T05:42:13.643914Z","shell.execute_reply":"2024-02-18T05:43:31.503634Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a0471e7a44440cb89cf21f113382a43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"353ac0bc592b4d1d84acdcf8a3ac941f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cdd94faea9548be9e4eef52e7de5782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c9dd82cf1674461a25a430282ef89b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"619872e2b2e745b6ad1ce6dfce6d02ba"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TFBertModel, BertTokenizer\nfrom tensorflow.keras.layers import Input, Dense, Concatenate\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\nfrom transformers import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nMAX_LEN = 64\n\n# 分词并转换为标记ID\ninput_ids = [tokenizer.encode(text, add_special_tokens=True) for text in nlp_data['Text']]\n\n# 截断/填充句子\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n                          value=0, truncating=\"post\", padding=\"post\")\n\n# 3. 注意力掩码准备\nattention_masks = []\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)\n    \ninput_ids_ = input_ids.copy()\nattention_masks_ = attention_masks.copy()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:43:31.505840Z","iopub.execute_input":"2024-02-18T05:43:31.506128Z","iopub.status.idle":"2024-02-18T05:44:01.382721Z","shell.execute_reply.started":"2024-02-18T05:43:31.506103Z","shell.execute_reply":"2024-02-18T05:44:01.381745Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 1. 数据分割\ntrain_inputs, validation_test_inputs, train_labels, validation_test_labels = train_test_split(input_ids_, nlp_data['Recommended IND'], \n                                                                                    random_state=2018, test_size=0.3)\nvalidation_inputs, test_inputs, validation_labels, test_labels = train_test_split(validation_test_inputs, validation_test_labels, \n                                                                                    random_state=2019, test_size=0.66)\ntrain_masks, validation_test_masks, _, _ = train_test_split(attention_masks_, input_ids_,\n                                                       random_state=2018, test_size=0.3)\n\nvalidation_masks, test_masks, _, _ = train_test_split(validation_test_masks, validation_test_inputs,\n                                                       random_state=2019, test_size=0.66)\n\n# 使用相同的分割索引分割ML数据\ntrain_ml_data, validation_test_ml_data,= train_test_split(ml_data.drop(['Recommended IND'],axis = 1).copy(), test_size=0.3, random_state=2018)\n\nvalidation_ml_data, test_ml_data,= train_test_split(validation_test_ml_data, test_size=0.66, random_state=2019)\n\nprint(train_inputs.shape)\n\nprint(validation_inputs.shape)\n\nprint(test_inputs.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:44:01.384370Z","iopub.execute_input":"2024-02-18T05:44:01.384681Z","iopub.status.idle":"2024-02-18T05:44:01.419703Z","shell.execute_reply.started":"2024-02-18T05:44:01.384655Z","shell.execute_reply":"2024-02-18T05:44:01.418593Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(13763, 64)\n(2005, 64)\n(3894, 64)\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2. 转换为Tensor\ntrain_inputs = tf.convert_to_tensor(train_inputs)\nvalidation_inputs = tf.convert_to_tensor(validation_inputs)\ntest_inputs = tf.convert_to_tensor(test_inputs)\n\ntrain_labels = tf.convert_to_tensor(train_labels)\nvalidation_labels = tf.convert_to_tensor(validation_labels)\ntest_labels = tf.convert_to_tensor(test_labels)\n\ntrain_masks = tf.convert_to_tensor(train_masks)\nvalidation_masks = tf.convert_to_tensor(validation_masks)\ntest_masks = tf.convert_to_tensor(test_masks)\n\ntrain_ml_data_ml = train_ml_data.astype(float)\nvalidation_ml_data_ml = validation_ml_data.astype(float)\ntest_ml_data_ml = test_ml_data.astype(float)\n\ntrain_ml_data_tensor = tf.convert_to_tensor(train_ml_data_ml.values)\nvalidation_ml_data_tensor = tf.convert_to_tensor(validation_ml_data_ml.values)\ntest_ml_data_tensor = tf.convert_to_tensor(test_ml_data_ml.values)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:44:01.421153Z","iopub.execute_input":"2024-02-18T05:44:01.421509Z","iopub.status.idle":"2024-02-18T05:44:01.835572Z","shell.execute_reply.started":"2024-02-18T05:44:01.421477Z","shell.execute_reply":"2024-02-18T05:44:01.834586Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 4. Modeling","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\n\ndef training(model, epochs):\n    # 配置模型检查点\n    model.summary()\n    checkpoint = ModelCheckpoint(\n        'model_checkpoint.h5',  # 模型保存路径和文件名\n        monitor='val_loss',  # 监控验证集损失\n        verbose=1,  # 输出保存信息\n        save_best_only=True,  # 只保存最佳模型\n        mode='min'  # 保存最小化指标的模型\n    )\n\n\n    # 配置早停\n    early_stopping = EarlyStopping(\n        monitor='val_loss',  # 监控验证集损失\n        patience=10,  # 2轮无提升则停止\n        restore_best_weights=True  # 恢复到最佳模型\n    )\n\n    # 训练模型\n    history = model.fit(\n        [train_inputs, train_masks, train_ml_data_tensor], train_labels,\n        batch_size=32,\n        epochs=epochs,  # 可以设置一个较大的数，因为早停会在训练过程中终止\n        validation_data=([validation_inputs, validation_masks, validation_ml_data_tensor], validation_labels),\n        callbacks=[early_stopping,checkpoint]  # 使用早停回调\n    )\n    return model\n\ndef prediction(model):\n    # 对验证集进行预测\n    predictions = model.predict([test_inputs, test_masks, test_ml_data_tensor])\n\n    # 计算性能指标\n    y_pred = np.round(predictions).flatten()\n    y_true = test_labels.numpy()  # 如果validation_labels是Tensor\n\n    recall = recall_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n\n    print(f\"Recall: {recall}\\nPrecision: {precision}\\nF1 Score: {f1}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:44:01.838023Z","iopub.execute_input":"2024-02-18T05:44:01.838896Z","iopub.status.idle":"2024-02-18T05:44:01.848617Z","shell.execute_reply.started":"2024-02-18T05:44:01.838858Z","shell.execute_reply":"2024-02-18T05:44:01.847582Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def RNN_Model():\n    # Define input layers for text input (sequence of word indices) and attention masks\n    input_ids = Input(shape=(64,), dtype=tf.int32, name='input_ids') \n    attention_masks = Input(shape=(64,), dtype=tf.int32, name='attention_masks')\n    \n    embedding_layer = Embedding(input_dim=len(tokenizer.vocab), output_dim=3, input_length=64)(input_ids)\n    rnn_output = SimpleRNN(10)(embedding_layer)  # SimpleRNN layer\n    \n    rnn_output = Dense(10, activation='relu')(rnn_output)\n\n\n    # Define input layer for non-text features\n    ml_input = Input(shape=(14,), name='ml_input')\n    \n    # Apply dense layers and dropout to non-text features\n    ml_output = Dense(64, activation='relu')(ml_input)\n    ml_output = Dropout(0.5)(ml_output)  \n    ml_output = Dense(32, activation='relu')(ml_output)  \n    ml_output = Dense(16, activation='relu')(ml_output)  \n\n    # Concatenate outputs from RNN and non-text feature branches\n    combined = Concatenate()([rnn_output, ml_output])\n    \n    # Apply dense layers to combined features\n    combined_output = Dense(64, activation='relu')(combined)\n    combined_output = Dense(1, activation='sigmoid')(combined_output)\n\n    # Build and compile the model\n    model = Model(inputs=[input_ids, attention_masks, ml_input], outputs=combined_output)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:46:02.045811Z","iopub.execute_input":"2024-02-18T05:46:02.046172Z","iopub.status.idle":"2024-02-18T05:46:02.055740Z","shell.execute_reply.started":"2024-02-18T05:46:02.046144Z","shell.execute_reply":"2024-02-18T05:46:02.054632Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def LSTM_Model():\n    # Define input layers for text input (sequence of word indices) and attention masks\n    input_ids = Input(shape=(64,), dtype=tf.int32, name='input_ids') \n    attention_masks = Input(shape=(64,), dtype=tf.int32, name='attention_masks')\n    \n    embedding_layer = Embedding(input_dim=len(tokenizer.vocab), output_dim=3, input_length=64)(input_ids)\n    lstm_output = Bidirectional(LSTM(10, return_sequences=True))(embedding_layer)\n    lstm_output = LSTM(10)(lstm_output)  # You can add more LSTM layers if needed\n    \n    # Apply dense layers and dropout to LSTM output\n    lstm_output = Dense(32, activation='relu')(lstm_output)\n    lstm_output = Dropout(0.4)(lstm_output)\n    lstm_output = Dense(4, activation='relu')(lstm_output)\n\n    # Define input layer for non-text features\n    ml_input = Input(shape=(14,), name='ml_input')\n    \n    ml_output = Dense(3, activation='relu')(ml_input)  \n\n    # Concatenate outputs from LSTM and non-text feature branches\n    combined = Concatenate()([lstm_output, ml_output])\n    \n    # Apply dense layers to combined features\n    combined_output = Dense(64, activation='relu')(combined)\n    combined_output = Dense(1, activation='sigmoid')(combined_output)\n\n    # Build and compile the model\n    model = Model(inputs=[input_ids, attention_masks, ml_input], outputs=combined_output)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:44:01.870966Z","iopub.execute_input":"2024-02-18T05:44:01.872043Z","iopub.status.idle":"2024-02-18T05:44:01.883094Z","shell.execute_reply.started":"2024-02-18T05:44:01.871995Z","shell.execute_reply":"2024-02-18T05:44:01.882155Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def GRU_Model():\n    # Define input layers for text input (sequence of word indices) and attention masks\n    input_ids = Input(shape=(64,), dtype=tf.int32, name='input_ids') \n    attention_masks = Input(shape=(64,), dtype=tf.int32, name='attention_masks')\n    \n    embedding_layer = Embedding(input_dim=len(tokenizer.vocab), output_dim=3, input_length=64)(input_ids)\n    gru_output = GRU(4)(embedding_layer)  # GRU layer\n    \n    gru_output = Dense(1, activation='relu')(gru_output)\n\n    # Define input layer for non-text features\n    ml_input = Input(shape=(14,), name='ml_input')\n\n    ml_output = Dense(4, activation='relu')(ml_input)  \n\n    # Concatenate outputs from GRU and non-text feature branches\n    combined = Concatenate()([gru_output, ml_output])\n    \n    # Apply dense layers to combined features\n    combined_output = Dense(10, activation='relu')(combined)\n    combined_output = Dense(1, activation='sigmoid')(combined_output)\n\n    # Build and compile the model\n    model = Model(inputs=[input_ids, attention_masks, ml_input], outputs=combined_output)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:44:01.884342Z","iopub.execute_input":"2024-02-18T05:44:01.884628Z","iopub.status.idle":"2024-02-18T05:44:01.898213Z","shell.execute_reply.started":"2024-02-18T05:44:01.884604Z","shell.execute_reply":"2024-02-18T05:44:01.897285Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def BERT_Model():\n    input_ids = Input(shape=(64,), dtype=tf.int32, name='input_ids') \n    attention_masks = Input(shape=(64,), dtype=tf.int32, name='attention_masks')\n    bert_output = bert(input_ids, attention_mask=attention_masks)[1]\n    bert_output = Dense(32, activation='relu')(bert_output)\n    bert_output = Dropout(0.4)(bert_output)\n    bert_output = Dense(32, activation='relu')(bert_output)\n\n    for layer in bert.layers:\n        layer.trainable = False\n\n    # 4.ML数据分支\n    ml_input = Input(shape=(14,), name='ml_input') # num_ml_features为ML数据的特征数量\n    ml_output = Dense(64, activation='relu')(ml_input)\n    ml_output = Dropout(0.5)(ml_output)  \n    ml_output = Dense(32, activation='relu')(ml_output)  \n    ml_output = Dense(16, activation='relu')(ml_output)  \n\n    # 合并两个分支\n    combined = Concatenate()([bert_output, ml_output])\n    combined_output = Dense(64, activation='relu')(combined)\n    combined_output = Dense(1, activation='sigmoid')(combined_output)\n\n    # 构建和编译模型\n    model = Model(inputs=[input_ids, attention_masks, ml_input], outputs=combined_output)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:44:01.899647Z","iopub.execute_input":"2024-02-18T05:44:01.899956Z","iopub.status.idle":"2024-02-18T05:44:01.913194Z","shell.execute_reply.started":"2024-02-18T05:44:01.899931Z","shell.execute_reply":"2024-02-18T05:44:01.912236Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# 5. RNN","metadata":{}},{"cell_type":"code","source":"rnn_model = training(RNN_Model(),2)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:55:27.033741Z","iopub.execute_input":"2024-02-18T05:55:27.034557Z","iopub.status.idle":"2024-02-18T05:56:38.357378Z","shell.execute_reply.started":"2024-02-18T05:55:27.034526Z","shell.execute_reply":"2024-02-18T05:56:38.356599Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Model: \"model_6\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n ml_input (InputLayer)       [(None, 14)]                 0         []                            \n                                                                                                  \n input_ids (InputLayer)      [(None, 64)]                 0         []                            \n                                                                                                  \n dense_36 (Dense)            (None, 64)                   960       ['ml_input[0][0]']            \n                                                                                                  \n embedding_5 (Embedding)     (None, 64, 3)                91566     ['input_ids[0][0]']           \n                                                                                                  \n dropout_44 (Dropout)        (None, 64)                   0         ['dense_36[0][0]']            \n                                                                                                  \n simple_rnn_3 (SimpleRNN)    (None, 10)                   140       ['embedding_5[0][0]']         \n                                                                                                  \n dense_37 (Dense)            (None, 32)                   2080      ['dropout_44[0][0]']          \n                                                                                                  \n dense_35 (Dense)            (None, 10)                   110       ['simple_rnn_3[0][0]']        \n                                                                                                  \n dense_38 (Dense)            (None, 16)                   528       ['dense_37[0][0]']            \n                                                                                                  \n concatenate_6 (Concatenate  (None, 26)                   0         ['dense_35[0][0]',            \n )                                                                   'dense_38[0][0]']            \n                                                                                                  \n dense_39 (Dense)            (None, 64)                   1728      ['concatenate_6[0][0]']       \n                                                                                                  \n attention_masks (InputLaye  [(None, 64)]                 0         []                            \n r)                                                                                               \n                                                                                                  \n dense_40 (Dense)            (None, 1)                    65        ['dense_39[0][0]']            \n                                                                                                  \n==================================================================================================\nTotal params: 97177 (379.60 KB)\nTrainable params: 97177 (379.60 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\nEpoch 1/2\n431/431 [==============================] - ETA: 0s - loss: 0.4558 - accuracy: 0.8188\nEpoch 1: val_loss improved from inf to 0.28836, saving model to model_checkpoint.h5\n431/431 [==============================] - 45s 97ms/step - loss: 0.4558 - accuracy: 0.8188 - val_loss: 0.2884 - val_accuracy: 0.8713\nEpoch 2/2\n  2/431 [..............................] - ETA: 23s - loss: 0.3246 - accuracy: 0.8750","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"431/431 [==============================] - ETA: 0s - loss: 0.2817 - accuracy: 0.8797\nEpoch 2: val_loss improved from 0.28836 to 0.24555, saving model to model_checkpoint.h5\n431/431 [==============================] - 26s 60ms/step - loss: 0.2817 - accuracy: 0.8797 - val_loss: 0.2456 - val_accuracy: 0.9022\n","output_type":"stream"}]},{"cell_type":"code","source":"prediction(rnn_model)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:56:51.547742Z","iopub.execute_input":"2024-02-18T05:56:51.548121Z","iopub.status.idle":"2024-02-18T05:56:52.469584Z","shell.execute_reply.started":"2024-02-18T05:56:51.548091Z","shell.execute_reply":"2024-02-18T05:56:52.468695Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"122/122 [==============================] - 1s 5ms/step\nRecall: 0.9041052961454089\nPrecision: 0.9789616559212758\nF1 Score: 0.9400456174649723\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 6. LSTM","metadata":{"execution":{"iopub.status.busy":"2024-02-18T04:35:54.711118Z","iopub.execute_input":"2024-02-18T04:35:54.711516Z","iopub.status.idle":"2024-02-18T04:35:54.715694Z","shell.execute_reply.started":"2024-02-18T04:35:54.711486Z","shell.execute_reply":"2024-02-18T04:35:54.714716Z"}}},{"cell_type":"code","source":"lstm_model = training(LSTM_Model(),2)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:48:53.409727Z","iopub.execute_input":"2024-02-18T05:48:53.410602Z","iopub.status.idle":"2024-02-18T05:49:37.248168Z","shell.execute_reply.started":"2024-02-18T05:48:53.410571Z","shell.execute_reply":"2024-02-18T05:49:37.247151Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model: \"model_3\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_ids (InputLayer)      [(None, 64)]                 0         []                            \n                                                                                                  \n embedding_3 (Embedding)     (None, 64, 3)                91566     ['input_ids[0][0]']           \n                                                                                                  \n bidirectional (Bidirection  (None, 64, 20)               1120      ['embedding_3[0][0]']         \n al)                                                                                              \n                                                                                                  \n lstm_1 (LSTM)               (None, 10)                   1240      ['bidirectional[0][0]']       \n                                                                                                  \n dense_19 (Dense)            (None, 32)                   352       ['lstm_1[0][0]']              \n                                                                                                  \n dropout_41 (Dropout)        (None, 32)                   0         ['dense_19[0][0]']            \n                                                                                                  \n ml_input (InputLayer)       [(None, 14)]                 0         []                            \n                                                                                                  \n dense_20 (Dense)            (None, 4)                    132       ['dropout_41[0][0]']          \n                                                                                                  \n dense_21 (Dense)            (None, 3)                    45        ['ml_input[0][0]']            \n                                                                                                  \n concatenate_3 (Concatenate  (None, 7)                    0         ['dense_20[0][0]',            \n )                                                                   'dense_21[0][0]']            \n                                                                                                  \n dense_22 (Dense)            (None, 64)                   512       ['concatenate_3[0][0]']       \n                                                                                                  \n attention_masks (InputLaye  [(None, 64)]                 0         []                            \n r)                                                                                               \n                                                                                                  \n dense_23 (Dense)            (None, 1)                    65        ['dense_22[0][0]']            \n                                                                                                  \n==================================================================================================\nTotal params: 95032 (371.22 KB)\nTrainable params: 95032 (371.22 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\nEpoch 1/2\n431/431 [==============================] - ETA: 0s - loss: 0.4136 - accuracy: 0.8195\nEpoch 1: val_loss improved from inf to 0.30591, saving model to model_checkpoint.h5\n431/431 [==============================] - 34s 62ms/step - loss: 0.4136 - accuracy: 0.8195 - val_loss: 0.3059 - val_accuracy: 0.8628\nEpoch 2/2\n  1/431 [..............................] - ETA: 1:02 - loss: 0.3186 - accuracy: 0.8750","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"431/431 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.8961\nEpoch 2: val_loss improved from 0.30591 to 0.18620, saving model to model_checkpoint.h5\n431/431 [==============================] - 9s 21ms/step - loss: 0.2381 - accuracy: 0.8961 - val_loss: 0.1862 - val_accuracy: 0.9192\n","output_type":"stream"}]},{"cell_type":"code","source":"prediction(lstm_model)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:50:06.378298Z","iopub.execute_input":"2024-02-18T05:50:06.378672Z","iopub.status.idle":"2024-02-18T05:50:08.041571Z","shell.execute_reply.started":"2024-02-18T05:50:06.378643Z","shell.execute_reply":"2024-02-18T05:50:08.040637Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"122/122 [==============================] - 2s 5ms/step\nRecall: 0.9952992792228141\nPrecision: 0.8986983588002264\nF1 Score: 0.9445353159851302\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 7. GRU","metadata":{"execution":{"iopub.status.busy":"2024-02-18T04:36:42.977278Z","iopub.execute_input":"2024-02-18T04:36:42.977978Z","iopub.status.idle":"2024-02-18T04:36:42.982117Z","shell.execute_reply.started":"2024-02-18T04:36:42.977943Z","shell.execute_reply":"2024-02-18T04:36:42.981109Z"}}},{"cell_type":"code","source":"gru_model = training(GRU_Model(),2)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:50:11.050776Z","iopub.execute_input":"2024-02-18T05:50:11.051153Z","iopub.status.idle":"2024-02-18T05:50:43.767586Z","shell.execute_reply.started":"2024-02-18T05:50:11.051123Z","shell.execute_reply":"2024-02-18T05:50:43.766656Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Model: \"model_4\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_ids (InputLayer)      [(None, 64)]                 0         []                            \n                                                                                                  \n embedding_4 (Embedding)     (None, 64, 3)                91566     ['input_ids[0][0]']           \n                                                                                                  \n gru (GRU)                   (None, 4)                    108       ['embedding_4[0][0]']         \n                                                                                                  \n ml_input (InputLayer)       [(None, 14)]                 0         []                            \n                                                                                                  \n dense_24 (Dense)            (None, 1)                    5         ['gru[0][0]']                 \n                                                                                                  \n dense_25 (Dense)            (None, 4)                    60        ['ml_input[0][0]']            \n                                                                                                  \n concatenate_4 (Concatenate  (None, 5)                    0         ['dense_24[0][0]',            \n )                                                                   'dense_25[0][0]']            \n                                                                                                  \n dense_26 (Dense)            (None, 10)                   60        ['concatenate_4[0][0]']       \n                                                                                                  \n attention_masks (InputLaye  [(None, 64)]                 0         []                            \n r)                                                                                               \n                                                                                                  \n dense_27 (Dense)            (None, 1)                    11        ['dense_26[0][0]']            \n                                                                                                  \n==================================================================================================\nTotal params: 91810 (358.63 KB)\nTrainable params: 91810 (358.63 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\nEpoch 1/2\n431/431 [==============================] - ETA: 0s - loss: 1.3571 - accuracy: 0.6502\nEpoch 1: val_loss improved from inf to 0.38580, saving model to model_checkpoint.h5\n431/431 [==============================] - 26s 53ms/step - loss: 1.3571 - accuracy: 0.6502 - val_loss: 0.3858 - val_accuracy: 0.8200\nEpoch 2/2\n 18/431 [>.............................] - ETA: 2s - loss: 0.3670 - accuracy: 0.8351","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"431/431 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.8644\nEpoch 2: val_loss improved from 0.38580 to 0.23702, saving model to model_checkpoint.h5\n431/431 [==============================] - 6s 15ms/step - loss: 0.3075 - accuracy: 0.8644 - val_loss: 0.2370 - val_accuracy: 0.9032\n","output_type":"stream"}]},{"cell_type":"code","source":"prediction(gru_model)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:51:07.707122Z","iopub.execute_input":"2024-02-18T05:51:07.707500Z","iopub.status.idle":"2024-02-18T05:51:08.465065Z","shell.execute_reply.started":"2024-02-18T05:51:07.707471Z","shell.execute_reply":"2024-02-18T05:51:08.464141Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"122/122 [==============================] - 1s 2ms/step\nRecall: 0.9899717956753369\nPrecision: 0.8916172734970365\nF1 Score: 0.9382239382239382\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 8. BERT","metadata":{"execution":{"iopub.status.busy":"2024-02-18T04:37:36.126561Z","iopub.execute_input":"2024-02-18T04:37:36.127279Z","iopub.status.idle":"2024-02-18T04:37:36.131208Z","shell.execute_reply.started":"2024-02-18T04:37:36.127247Z","shell.execute_reply":"2024-02-18T04:37:36.130175Z"}}},{"cell_type":"code","source":"bert_model = training(BERT_Model(),4)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:51:15.771093Z","iopub.execute_input":"2024-02-18T05:51:15.771471Z","iopub.status.idle":"2024-02-18T05:54:58.624096Z","shell.execute_reply.started":"2024-02-18T05:51:15.771442Z","shell.execute_reply":"2024-02-18T05:54:58.623227Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Model: \"model_5\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_ids (InputLayer)      [(None, 64)]                 0         []                            \n                                                                                                  \n attention_masks (InputLaye  [(None, 64)]                 0         []                            \n r)                                                                                               \n                                                                                                  \n ml_input (InputLayer)       [(None, 14)]                 0         []                            \n                                                                                                  \n tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1094822   ['input_ids[0][0]',           \n )                           ngAndCrossAttentions(last_   40         'attention_masks[0][0]']     \n                             hidden_state=(None, 64, 76                                           \n                             8),                                                                  \n                              pooler_output=(None, 768)                                           \n                             , past_key_values=None, hi                                           \n                             dden_states=None, attentio                                           \n                             ns=None, cross_attentions=                                           \n                             None)                                                                \n                                                                                                  \n dense_30 (Dense)            (None, 64)                   960       ['ml_input[0][0]']            \n                                                                                                  \n dense_28 (Dense)            (None, 32)                   24608     ['tf_bert_model[0][1]']       \n                                                                                                  \n dropout_43 (Dropout)        (None, 64)                   0         ['dense_30[0][0]']            \n                                                                                                  \n dropout_42 (Dropout)        (None, 32)                   0         ['dense_28[0][0]']            \n                                                                                                  \n dense_31 (Dense)            (None, 32)                   2080      ['dropout_43[0][0]']          \n                                                                                                  \n dense_29 (Dense)            (None, 32)                   1056      ['dropout_42[0][0]']          \n                                                                                                  \n dense_32 (Dense)            (None, 16)                   528       ['dense_31[0][0]']            \n                                                                                                  \n concatenate_5 (Concatenate  (None, 48)                   0         ['dense_29[0][0]',            \n )                                                                   'dense_32[0][0]']            \n                                                                                                  \n dense_33 (Dense)            (None, 64)                   3136      ['concatenate_5[0][0]']       \n                                                                                                  \n dense_34 (Dense)            (None, 1)                    65        ['dense_33[0][0]']            \n                                                                                                  \n==================================================================================================\nTotal params: 109514673 (417.77 MB)\nTrainable params: 32433 (126.69 KB)\nNon-trainable params: 109482240 (417.64 MB)\n__________________________________________________________________________________________________\nEpoch 1/4\n430/431 [============================>.] - ETA: 0s - loss: 0.5066 - accuracy: 0.8089\nEpoch 1: val_loss improved from inf to 0.44239, saving model to model_checkpoint.h5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"431/431 [==============================] - 68s 123ms/step - loss: 0.5065 - accuracy: 0.8089 - val_loss: 0.4424 - val_accuracy: 0.8185\nEpoch 2/4\n430/431 [============================>.] - ETA: 0s - loss: 0.3698 - accuracy: 0.8388\nEpoch 2: val_loss improved from 0.44239 to 0.22735, saving model to model_checkpoint.h5\n431/431 [==============================] - 49s 114ms/step - loss: 0.3698 - accuracy: 0.8388 - val_loss: 0.2274 - val_accuracy: 0.9022\nEpoch 3/4\n430/431 [============================>.] - ETA: 0s - loss: 0.2504 - accuracy: 0.8964\nEpoch 3: val_loss improved from 0.22735 to 0.16925, saving model to model_checkpoint.h5\n431/431 [==============================] - 49s 114ms/step - loss: 0.2504 - accuracy: 0.8963 - val_loss: 0.1693 - val_accuracy: 0.9272\nEpoch 4/4\n431/431 [==============================] - ETA: 0s - loss: 0.2063 - accuracy: 0.9154\nEpoch 4: val_loss improved from 0.16925 to 0.15406, saving model to model_checkpoint.h5\n431/431 [==============================] - 49s 114ms/step - loss: 0.2063 - accuracy: 0.9154 - val_loss: 0.1541 - val_accuracy: 0.9362\n","output_type":"stream"}]},{"cell_type":"code","source":"prediction(bert_model)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T05:55:00.645218Z","iopub.execute_input":"2024-02-18T05:55:00.646356Z","iopub.status.idle":"2024-02-18T05:55:14.877472Z","shell.execute_reply.started":"2024-02-18T05:55:00.646313Z","shell.execute_reply":"2024-02-18T05:55:14.876465Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"122/122 [==============================] - 14s 90ms/step\nRecall: 0.9486054528361015\nPrecision: 0.9646271510516252\nF1 Score: 0.956549217885922\n","output_type":"stream"}]}]}